{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-killing",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loaded-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import gen_id_dated, gdf_viewer, gdf_geom, gdf_merger, gdf_filter, na_col_drop, na_line_drop\n",
    "import re, os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "#from shapely.geometry import Point\n",
    "import datetime as dtm\n",
    "import matplotlib.pyplot as plt\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "potential-pride",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def files_search(work_dir, files_dict, prefix='', skip=None, details=False):   \n",
    "    \n",
    "    if skip is None:\n",
    "        skip = \"we don't want to skip a word\"\n",
    "        \n",
    "    for k in files_dict.keys():\n",
    "        tmp_list = []\n",
    "        for p,d,f in os.walk(work_dir):\n",
    "            for x in f:\n",
    "                add = False\n",
    "                if re.search(prefix,x,re.I) and not re.search(skip,x,re.I):\n",
    "                    add = True\n",
    "                    i = str(f'{p}/{x}')\n",
    "                else:\n",
    "                    add = False\n",
    "                    i=''\n",
    "                    \n",
    "                if re.search(k,i,re.I) and add:\n",
    "                    tmp_list.append(i)\n",
    "        tmp_list.sort()\n",
    "        files_dict.update({k:tmp_list})\n",
    "\n",
    "    for k,v in files_dict.items():\n",
    "        print(k,' \\t: ',len(v))\n",
    "    \n",
    "    if details: # Look filenames\n",
    "        which = files_dict.keys()\n",
    "\n",
    "        for w in which:\n",
    "            print('\\n+++++++++++++++++')\n",
    "            print(f'+  {w.upper()}\\t+ ')\n",
    "            print('+++++++++++++++++')\n",
    "            [print(i, '-', x) for i, x in enumerate(files_dict[w], 0)]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_col(data):\n",
    "    cols_idx = []\n",
    "    \n",
    "    class DoubledColumns(Exception):\n",
    "        \"\"\"Merging process doubled column(s) still remain. Check and drop them before continue\"\"\"\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(data.columns)):\n",
    "        if re.search('_x|_y', list(data.columns)[i]):\n",
    "            cols_idx.append(i)\n",
    "    \n",
    "    if len(cols_idx) != 0 :\n",
    "        raise DoubledColumns(f'Merging process doubled column(s) still remain.'\n",
    "                             f'\\nCheck and drop them before continue ! Doubled columns position {cols_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chronic-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_obj_test(df1, df2, on='ID', how='outer', dist_max=1):\n",
    "    test_distinct = df1.merge(df2, on=on, how=how)\n",
    "\n",
    "    dist_max = dist_max\n",
    "    \n",
    "    if 'X' in list(df1.columns) and 'X' in list(df2.columns):\n",
    "        for idx in test_distinct.index:\n",
    "            distinct_objects = True\n",
    "            if not pd.isnull(test_distinct.loc[idx,'X_x']) and not pd.isnull(test_distinct.loc[idx,'X_y']):\n",
    "                dist = (test_distinct.loc[idx,'X_x'] - test_distinct.loc[idx,'X_y']) ** 2 + (test_distinct.loc[idx,'Y_x'] - test_distinct.loc[idx,'Y_y']) ** 2\n",
    "                if dist <= (dist_max) ** 2:  # consider as same object\n",
    "                    distinct_objects = False\n",
    "            else:\n",
    "                distinct_objects = False\n",
    "            test_distinct.loc[idx, 'Distinct_obj'] = distinct_objects\n",
    "\n",
    "        test_distinct.insert(1,'Distinct_obj', test_distinct.pop('Distinct_obj') )\n",
    "\n",
    "        gdf_viewer(test_distinct)\n",
    "    else:\n",
    "        print('Cannot proceed ! No position data in one of the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file1, file2): # find another name for this function\n",
    "    \"\"\"\n",
    "    create dataframes from files and test if they contain position informations\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_csv(file1, delimiter=',')\n",
    "    df2 = pd.read_csv(file2, delimiter=',')\n",
    "    \n",
    "    print(f\"df1 : {file1.replace(work_dir,'')} \\ndf2 : {file2.replace(work_dir,'')}\\n\")\n",
    "\n",
    "    if 'X' in list(df1.columns): print('df1 - Position data')\n",
    "    else: print('df1 - No position data')\n",
    "    if 'X' in list(df2.columns): print('df2 - Position data')\n",
    "    else: print('df2 - No position data')\n",
    "    \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cardiovascular-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, data_to_check, valid_data, col, idx_list, valid_col):\n",
    "    \n",
    "    old_idx_col = 'Source_index'\n",
    "    \n",
    "    for col, idx_list in valid_data.items():\n",
    "        if old_idx_col in data_to_check.columns:\n",
    "            idx = data_to_check.loc[i, old_idx_col]\n",
    "            data.loc[idx, col] = data_to_check.loc[i, valid_col]\n",
    "        else:\n",
    "            raise NameError(f\"Dataframe to check must contain a column named : '{old_idx_col}'!\")\n",
    "\n",
    "    data_to_check.drop(index=idx_list, inplace=True)\n",
    "    data_to_check.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Operation done \")\n",
    "    \n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_objects_check(data):\n",
    "    uniq_ID = []\n",
    "    dbl_ID = []\n",
    "    idx_ = []\n",
    "    qdf = pd.DataFrame()\n",
    "\n",
    "    for i in data.index:\n",
    "        id_ = data.loc[i, 'ID']\n",
    "\n",
    "        if id_ not in uniq_ID:\n",
    "            uniq_ID.append(id_)\n",
    "        elif id_ not in dbl_ID:\n",
    "            dbl_ID.append(id_)\n",
    "        else:\n",
    "            idx_.append(i)\n",
    "\n",
    "    for i in dbl_ID:\n",
    "        qdf = qdf.append(data.query(f\"ID=='{i}'\"))\n",
    "\n",
    "    return qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-noise",
   "metadata": {},
   "source": [
    "## Files reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vanilla-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = ROOT_DIR+'/CF_data/Result_traitem/'\n",
    "save_dir = ROOT_DIR+'/CF_data/Donnees_fusionnees/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confused-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my dictionary structure to retrieve good files (Keynames !!!)\n",
    "files_dict={'Borehole':0,'Piezometer':0,'Piezair':0,'Trench':0,'Litho':0,'Equipm':0,\n",
    "        'Measure':0,'Sample':0,'Analysis':0,'facility':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compliant-sensitivity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borehole  \t:  7\n",
      "Piezometer  \t:  17\n",
      "Piezair  \t:  2\n",
      "Trench  \t:  1\n",
      "Litho  \t:  7\n",
      "Equipm  \t:  3\n",
      "Measure  \t:  6\n",
      "Sample  \t:  27\n",
      "Analysis  \t:  21\n",
      "facility  \t:  4\n"
     ]
    }
   ],
   "source": [
    "files_search(work_dir, files_dict, prefix='', skip='source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "possible-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "how=['inner', 'outer', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "national-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = False\n",
    "t = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-transformation",
   "metadata": {},
   "source": [
    "# Boreholes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-oregon",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- correct extraction in the file 2 -> Samples\n",
    "- file 4 and file 5 are the same in result (check it)\n",
    "- try to concatenate file 1 with piezo (if possible because no position)\n",
    "- check processing for 'refus and 'type_refus' (every object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surprised-relations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/Forage_Pilote/leve_Z_elect_pos_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/Prof_contact_sol_forage/Feuil1_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/database_Memoris3/Profils_sol_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/donnees_terrain_2019/Donnees_forage_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/profils_sols_donnees_forages/Equipement_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/profils_sols_donnees_forages/Log_Boreholes.csv',\n",
       " '/home/yanathan/Projects/GSDMA/CF_data/Result_traitem/profils_sols_donnees_forages/donnees_forage_Boreholes.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys: Borehole','Piezometer','Litho', 'Trench','Equipm','Measure','Sample','Analysis','facility'\n",
    "files_dict['Borehole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "champion-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 files\n"
     ]
    }
   ],
   "source": [
    "key='Borehole'\n",
    "save_file = f'Merged_Boreholes.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus', 'Societe'] #columns of interest\n",
    "boreholes = pd.DataFrame() # for saving object info after last merging\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stable-stranger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 : Prof_contact_sol_forage/Feuil1_Boreholes.csv \n",
      "df2 : donnees_terrain_2019/Donnees_forage_Boreholes.csv\n",
      "\n",
      "df1 - Position data\n",
      "df2 - Position data\n",
      "Rows : 8, columns : 6, Unique col 'ID': 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086c47cca5104a4bbf9776ba381ee238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=8, min=3, readout=False), IntSlider(value=6, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 16, columns : 18, Unique col 'ID': 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f70051bca47d9af91fdab5ee7506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=16, min=3, readout=False), IntSlider(value=12…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1= files_dict[key][1]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "configured-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.ID=df2.ID.apply(lambda x: 'F'+x) # name recent (2019) boreholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personalized-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 17, columns : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34da4e8806b4dfb8ba8880d8f48249c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='rows', max=17, min=10, readout=False), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promotional-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous values present. Please resolve this manually !\n"
     ]
    }
   ],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "continuing-bridal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Long_for_x</th>\n",
       "      <th>Long_for_y</th>\n",
       "      <th>Source_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F205</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F208</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F212</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F207</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F214</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F217</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F225</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Long_for_x  Long_for_y  Source_index\n",
       "0  F205         3.2         4.8             0\n",
       "1  F208         3.4         4.8             1\n",
       "2  F212         3.4         4.8             2\n",
       "3  F207         3.4         4.8             3\n",
       "4  F214         3.6         4.8             4\n",
       "5  F217         4.2         4.8             5\n",
       "6  F225         4.0         4.8             6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescribed-block",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 17, columns : 20, Unique col 'ID': 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe337aa3ca840bfa4c86813fac117e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='rows', max=17, min=10, readout=False), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-transport",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "criminal-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chronic-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 : database_Memoris3/Profils_sol_Boreholes.csv \n",
      "df2 : profils_sols_donnees_forages/Equipement_Boreholes.csv\n",
      "\n",
      "df1 - No position data\n",
      "df2 - Position data\n",
      "Rows : 172, columns : 6, Unique col 'ID': 172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ac7e0ee36b41da8b7d28f6e495b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=172, min=3, readout=False), IntSlider(value=6…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 13, columns : 13, Unique col 'ID': 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9875e6a3e3640e0a31c666a94ea12c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=13, min=3, readout=False), IntSlider(value=12…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protecting-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot proceed ! No position data in one of the dataframe\n"
     ]
    }
   ],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aggregate-tower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "major-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['ID_date'] = mdf['ID_date'].apply(lambda x: str(x).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accepted-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "crucial-keyboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 185, columns : 13, Unique col 'ID': 185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0610dbdef7854f9f8a2bcec77e43ff6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=185, min=3, readout=False), IntSlider(value=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-chicken",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "serious-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([('s', 'p', 'l', 'i', 't', '_', 'd', 'i', 's', 't', 'i', 'n', 'c', 't'), ('s', 'p', 'l', 'i', 't', '_', 'd', 'i', 's', 't', 'i', 'n', 'c', 't')], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fd4e7f5494fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboreholes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgdf_merger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboreholes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcheck_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboreholes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/GSDMA/utils/io.py\u001b[0m in \u001b[0;36mgdf_merger\u001b[0;34m(gdf1, gdf2, how, on, left_on, right_on, dist_max, verbose)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0mgdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdble_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'split_distinct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0mgdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m     \u001b[0mgdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split_distinct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconflict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, key, axis, is_setter)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-DRfwm83x/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([('s', 'p', 'l', 'i', 't', '_', 'd', 'i', 's', 't', 'i', 'n', 'c', 't'), ('s', 'p', 'l', 'i', 't', '_', 'd', 'i', 's', 't', 'i', 'n', 'c', 't')], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID', dist_max=2)\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(boreholes, err_df, on='ID', col='Long_for', idx_list=[0,1,2], valid_col='Long_for_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "expired-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-classics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][0]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', )\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-hours",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-housing",
   "metadata": {},
   "source": [
    "#### Last boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-feature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID')\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-press",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_df # i think there are not the same, but no date or postition to distinguish them !\n",
    "# --> check boreholes sheets (pdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hungarian-partition",
   "metadata": {},
   "source": [
    "validate_data(boreholes, err_df, on='ID', col='Long_for', idx_list=[], valid_col='Long_for_y' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "parliamentary-newark",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-portrait",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Boreholes data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "boreholes.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-flexibility",
   "metadata": {},
   "source": [
    "# Piezometers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-usage",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Piezometer'\n",
    "save_file = f'Merged_Piezometers.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "piezometers = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-centre",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-garden",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-county",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-associate",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-burning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-midwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-chicago",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-sentence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-tyler",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [292, 293]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-shanghai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][4]\n",
    "file2= files_dict[key][5]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-essex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-duplicate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-whale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-differential",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][6]\n",
    "file2= files_dict[key][9]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ID'] = df2.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-dress",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-pulse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-ceramic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][10]\n",
    "file2= files_dict[key][11]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ID'] = df1.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-ireland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-catalog",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-battery",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "legislative-financing",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-broadcasting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-reserve",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][12]\n",
    "file2= files_dict[key][13]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-tuning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adjustable-marker",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "realistic-myrtle",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-geography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-sheffield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-stomach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][14]\n",
    "file2= files_dict[key][15]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-holder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-grove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "retained-marking",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "floral-script",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-lexington",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-colleague",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][16]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-stack",
   "metadata": {},
   "source": [
    "#### Last merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-alberta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "median-portrait",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "elder-weekly",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-dairy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lasting-issue",
   "metadata": {},
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "comparable-transformation",
   "metadata": {},
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "illegal-going",
   "metadata": {},
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-shock",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Piezometers data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "piezometers.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-orchestra",
   "metadata": {},
   "source": [
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-ranch",
   "metadata": {},
   "source": [
    "# Unknown facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='facility'\n",
    "save_file = f'Merged_Facilites_unknw.csv'\n",
    "#coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "facilities = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-beginning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-metabolism",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][1]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-listing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facilities, err_df=gdf_merger(facilities, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-campbell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(facilities, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-asset",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Unknown Facilities data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "facilities.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-clearance",
   "metadata": {},
   "source": [
    "# Lithologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-timeline",
   "metadata": {},
   "source": [
    "Do not add parameter 'dist_max' when merging without considering position !!! otherwise, unuseless rows added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Litho'\n",
    "save_file = f'Merged_Lithologies.csv'\n",
    "coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "lithologies = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-coverage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-catholic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = list(set(df1.columns) & set(df2.columns))\n",
    "test1 = df1.merge(df2, how = 'inner', on='ID')\n",
    "test2 = df1.merge(df2, how = 'outer', on='ID', indicator=True).loc[lambda x : x.query('_merge ==\"right_only\" or _merge==\"left_only\"').index]\n",
    "test3 = test1.merge(test2, how = 'outer', on='ID')\n",
    "test4 = df1.merge(df2, how = 'outer', on=list(common_cols))\n",
    "print((len(test1), len(test2), len(test3)))\n",
    "gdf_viewer(test4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wired-trade",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "yellow-medicare",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-manitoba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-snake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "respected-angle",
   "metadata": {},
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "structural-express",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-intelligence",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-preview",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-responsibility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=10, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-measure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][6]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how='inner', on='ID'), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how = 'outer', on='ID',indicator=True), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-medicaid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "initial-piano",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governing-spider",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-lodging",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-flower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "changing-accident",
   "metadata": {},
   "source": [
    "mdf = mdf.loc[mdf.query('Terrain==Terrain').index,:]\n",
    "mdf.drop(columns=['Terrain_x', 'Terrain_y'], inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lovely-ballot",
   "metadata": {},
   "source": [
    "data = mdf.copy()\n",
    "col = 'Terrain'\n",
    "\n",
    "id_list = []\n",
    "keep_idx = []\n",
    "\n",
    "for i in data.index:\n",
    "    _id = data.loc[i,'ID']\n",
    "    if re.search(\"'\", _id): \n",
    "        _id = f\"`{_id}`\" # it doesn't work \n",
    "    else:\n",
    "        _id = f\"{_id}\"\n",
    "    \n",
    "    if _id not in id_list:\n",
    "        id_list.append(_id)\n",
    "        tmp = data[data['ID']==f\"{_id}\"]\n",
    "\n",
    "        if len(tmp) < 2 and len(tmp) > 0:\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "        else:\n",
    "            tmp = tmp[tmp[col]==tmp[col]]\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "print(keep_idx)\n",
    "    \n",
    "data = data.loc[keep_idx,:]\n",
    "data.drop(columns=[col+'_x', col+'_y'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "chronic-greek",
   "metadata": {},
   "source": [
    "df1.drop(columns=['Societe','Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "warming-cleaner",
   "metadata": {},
   "source": [
    "df1['X'] = df1['X'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Y'] = df1['Y'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Ep_remb'] = df1['Ep_remb'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governmental-spyware",
   "metadata": {},
   "source": [
    "df1['Type'] = df1['Type'].astype('object')\n",
    "df1[['X','Y','Ep_remb']] = df1[['X','Y','Ep_remb']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-precipitation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-courage",
   "metadata": {},
   "source": [
    "# Equipments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-thesis",
   "metadata": {},
   "source": [
    "We must also retrieve equipments information from boreholes and piezometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Equipm'\n",
    "save_file = f'Merged_Equipments.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "equipments = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-garlic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-battlefield",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-doubt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-speaker",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-desire",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-density",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "promotional-joint",
   "metadata": {},
   "source": [
    "key='Samples'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "second-springfield",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "manual-talent",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrong-jersey",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-blood",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-chapel",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dental-hello",
   "metadata": {},
   "source": [
    "key='Measures'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "european-breast",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-tomato",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "optimum-concern",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-sewing",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-winner",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "productive-wheat",
   "metadata": {},
   "source": [
    "key='Analysis'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vulnerable-encounter",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "genuine-fraction",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "married-beatles",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-applicant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
