{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-killing",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loaded-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import gen_id_dated, gdf_viewer, gdf_geom, gdf_merger, gdf_filter, na_col_drop, na_line_drop\n",
    "import re, os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "#from shapely.geometry import Point\n",
    "import datetime as dtm\n",
    "import matplotlib.pyplot as plt\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "potential-pride",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def files_search(work_dir, files_dict, prefix='', skip=None, details=False):   \n",
    "    \n",
    "    if skip is None:\n",
    "        skip = \"we don't want to skip a word\"\n",
    "        \n",
    "    for k in files_dict.keys():\n",
    "        tmp_list = []\n",
    "        for p,d,f in os.walk(work_dir):\n",
    "            for x in f:\n",
    "                add = False\n",
    "                if re.search(prefix,x,re.I) and not re.search(skip,x,re.I):\n",
    "                    add = True\n",
    "                    i = str(f'{p}/{x}')\n",
    "                else:\n",
    "                    add = False\n",
    "                    i=''\n",
    "                    \n",
    "                if re.search(k,i,re.I) and add:\n",
    "                    tmp_list.append(i)\n",
    "        tmp_list.sort()\n",
    "        files_dict.update({k:tmp_list})\n",
    "\n",
    "    for k,v in files_dict.items():\n",
    "        print(k,' \\t: ',len(v))\n",
    "    \n",
    "    if details: # Look filenames\n",
    "        which = files_dict.keys()\n",
    "\n",
    "        for w in which:\n",
    "            print('\\n+++++++++++++++++')\n",
    "            print(f'+  {w.upper()}\\t+ ')\n",
    "            print('+++++++++++++++++')\n",
    "            [print(i, '-', x) for i, x in enumerate(files_dict[w], 0)]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_col(data):\n",
    "    cols_idx = []\n",
    "    \n",
    "    class DoubledColumns(Exception):\n",
    "        \"\"\"Merging process doubled column(s) still remain. Check and drop them before continue\"\"\"\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(data.columns)):\n",
    "        if re.search('_x|_y', list(data.columns)[i]):\n",
    "            cols_idx.append(i)\n",
    "    \n",
    "    if len(cols_idx) != 0 :\n",
    "        raise DoubledColumns(f'Merging process doubled column(s) still remain.'\n",
    "                             f'\\nCheck and drop them before continue ! Doubled columns position {cols_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chronic-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_obj_test(df1, df2, on='ID', how='outer', dist_max=1):\n",
    "    test_distinct = df1.merge(df2, on=on, how=how)\n",
    "\n",
    "    dist_max = dist_max\n",
    "    \n",
    "    if 'X' in list(df1.columns) and 'X' in list(df2.columns):\n",
    "        for idx in test_distinct.index:\n",
    "            distinct_objects = True\n",
    "            if not pd.isnull(test_distinct.loc[idx,'X_x']) and not pd.isnull(test_distinct.loc[idx,'X_y']):\n",
    "                dist = (test_distinct.loc[idx,'X_x'] - test_distinct.loc[idx,'X_y']) ** 2 + (test_distinct.loc[idx,'Y_x'] - test_distinct.loc[idx,'Y_y']) ** 2\n",
    "                if dist <= (dist_max) ** 2:  # consider as same object\n",
    "                    distinct_objects = False\n",
    "            else:\n",
    "                distinct_objects = False\n",
    "            test_distinct.loc[idx, 'Distinct_obj'] = distinct_objects\n",
    "\n",
    "        test_distinct.insert(1,'Distinct_obj', test_distinct.pop('Distinct_obj') )\n",
    "\n",
    "        gdf_viewer(test_distinct)\n",
    "    else:\n",
    "        print('Cannot proceed ! No position data in one of the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file1, file2): # find another name for this function\n",
    "    \"\"\"\n",
    "    create dataframes from files and test if they contain position informations\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_csv(file1, delimiter=',')\n",
    "    df2 = pd.read_csv(file2, delimiter=',')\n",
    "    \n",
    "    print(f\"df1 : {file1.replace(work_dir,'')} \\ndf2 : {file2.replace(work_dir,'')}\\n\")\n",
    "\n",
    "    if 'X' in list(df1.columns): print('df1 - Position data')\n",
    "    else: print('df1 - No position data')\n",
    "    if 'X' in list(df2.columns): print('df2 - Position data')\n",
    "    else: print('df2 - No position data')\n",
    "    \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cardiovascular-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, data_to_check, valid_data, col, idx_list, valid_col):\n",
    "    \n",
    "    old_idx_col = 'Source_index'\n",
    "    \n",
    "    for col, idx_list in valid_data.items():\n",
    "        if old_idx_col in data_to_check.columns:\n",
    "            idx = data_to_check.loc[i, old_idx_col]\n",
    "            data.loc[idx, col] = data_to_check.loc[i, valid_col]\n",
    "        else:\n",
    "            raise NameError(f\"Dataframe to check must contain a column named : '{old_idx_col}'!\")\n",
    "\n",
    "    data_to_check.drop(index=idx_list, inplace=True)\n",
    "    data_to_check.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Operation done \")\n",
    "    \n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_objects_check(data):\n",
    "    uniq_ID = []\n",
    "    dbl_ID = []\n",
    "    idx_ = []\n",
    "    qdf = pd.DataFrame()\n",
    "\n",
    "    for i in data.index:\n",
    "        id_ = data.loc[i, 'ID']\n",
    "\n",
    "        if id_ not in uniq_ID:\n",
    "            uniq_ID.append(id_)\n",
    "        elif id_ not in dbl_ID:\n",
    "            dbl_ID.append(id_)\n",
    "        else:\n",
    "            idx_.append(i)\n",
    "\n",
    "    for i in dbl_ID:\n",
    "        qdf = qdf.append(data.query(f\"ID=='{i}'\"))\n",
    "\n",
    "    return qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-noise",
   "metadata": {},
   "source": [
    "## Files reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vanilla-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = ROOT_DIR+'/CF_data/Result_traitem/'\n",
    "save_dir = ROOT_DIR+'/CF_data/Donnees_fusionnees/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confused-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my dictionary structure to retrieve good files (Keynames !!!)\n",
    "files_dict={'Borehole':0,'Piezometer':0,'Piezair':0,'Trench':0,'Litho':0,'Equipm':0,\n",
    "        'Measure':0,'Sample':0,'Analysis':0,'facility':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compliant-sensitivity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borehole  \t:  5\n",
      "Piezometer  \t:  14\n",
      "Piezair  \t:  1\n",
      "Trench  \t:  1\n",
      "Litho  \t:  6\n",
      "Equipm  \t:  2\n",
      "Measure  \t:  6\n",
      "Sample  \t:  26\n",
      "Analysis  \t:  20\n",
      "facility  \t:  4\n"
     ]
    }
   ],
   "source": [
    "files_search(work_dir, files_dict, prefix='', skip='source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "possible-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "how=['inner', 'outer', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "national-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = False\n",
    "t = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-transformation",
   "metadata": {},
   "source": [
    "# Boreholes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-oregon",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- correct extraction in the file 2 -> Samples\n",
    "- file 4 and file 5 are the same in result (check it)\n",
    "- try to concatenate file 1 with piezo (if possible because no position)\n",
    "- check processing for 'refus and 'type_refus' (every object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surprised-relations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/su530201/PycharmProjects/GSDMA/CF_data/Result_traitem/Forage_Pilote/leve_Z_elect_pos_Boreholes.csv',\n",
       " '/home/su530201/PycharmProjects/GSDMA/CF_data/Result_traitem/Prof_contact_sol_forage/Feuil1_Boreholes.csv',\n",
       " '/home/su530201/PycharmProjects/GSDMA/CF_data/Result_traitem/database_Memoris3/Profils_sol_Boreholes.csv',\n",
       " '/home/su530201/PycharmProjects/GSDMA/CF_data/Result_traitem/donnees_terrain_2019/Donnees_forage_Boreholes.csv',\n",
       " '/home/su530201/PycharmProjects/GSDMA/CF_data/Result_traitem/profils_sols_donnees_forages/donnees_forage_Boreholes.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys: Borehole','Piezometer','Litho', 'Trench','Equipm','Measure','Sample','Analysis','facility'\n",
    "files_dict['Borehole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "champion-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 files\n"
     ]
    }
   ],
   "source": [
    "key='Borehole'\n",
    "save_file = f'Merged_Boreholes.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus', 'Societe'] #columns of interest\n",
    "boreholes = pd.DataFrame() # for saving object info after last merging\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stable-stranger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 : Prof_contact_sol_forage/Feuil1_Boreholes.csv \n",
      "df2 : donnees_terrain_2019/Donnees_forage_Boreholes.csv\n",
      "\n",
      "df1 - Position data\n",
      "df2 - Position data\n",
      "Rows : 8, columns : 6, Unique col 'ID': 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bce8cb7c6b4fc39be402c512799712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=8, min=3, readout=False), IntSlider(value=6, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 16, columns : 18, Unique col 'ID': 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21e47a6b2de4039a79726e96783f3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=16, min=3, readout=False), IntSlider(value=12…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1= files_dict[key][1]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "configured-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.ID=df2.ID.apply(lambda x: 'F'+x) # name recent (2019) boreholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personalized-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 17, columns : 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025b1aa3fee8429b8468b6ae9b36c098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='rows', max=17, min=10, readout=False), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promotional-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index Type_refus    Methode  Crep_long     Resp_chantier  Diam_ext_pz  \\\n",
      "0       0        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "1       1        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "2       2        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "3       3        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "4       4        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "5       5        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "6       6        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "7       7        NaN        NaN        NaN               NaN          NaN   \n",
      "8       8      Béton  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "9       9        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "10     10      Béton  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "11     11        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "12     12      Béton  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "13     13        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "14     14        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "15     15        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "16     16        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "\n",
      "    Diam_int_pz Refus        ID  Diam_for    Date_ouv  Long_pz  \\\n",
      "0           NaN   NaN      F205      75.0  2019-10-07      NaN   \n",
      "1           NaN   NaN      F208      75.0  2019-10-07      NaN   \n",
      "2           NaN   NaN      F212      75.0  2019-10-07      NaN   \n",
      "3           NaN   NaN      F207      75.0  2019-10-07      NaN   \n",
      "4           NaN   NaN      F214      75.0  2019-10-07      NaN   \n",
      "5           NaN   NaN      F217      75.0  2019-10-07      NaN   \n",
      "6           NaN   NaN      F225      75.0  2019-10-08      NaN   \n",
      "7           NaN   NaN      F304       NaN         NaN      NaN   \n",
      "8           NaN     x      F201      75.0  2019-10-07      NaN   \n",
      "9           NaN     x      F221      75.0  2019-10-07      NaN   \n",
      "10          NaN     x      F223      75.0  2019-10-07      NaN   \n",
      "11          NaN     x  F224 bis      75.0  2019-10-08      NaN   \n",
      "12          NaN     x      F224      75.0  2019-10-08      NaN   \n",
      "13          NaN     x      F219      75.0  2019-10-08      NaN   \n",
      "14          NaN     x      F220      75.0  2019-10-08      NaN   \n",
      "15          NaN     x  F220 bis      75.0  2019-10-08      NaN   \n",
      "16          NaN   NaN      F226      75.0  2019-10-08      NaN   \n",
      "\n",
      "             Societe       ID_date  Long_for           Z              X  \\\n",
      "0   ECOPLANNING sprl      2019-205       NaN  101.804000  152887.693000   \n",
      "1   ECOPLANNING sprl      2019-208       NaN  101.848000  152885.296000   \n",
      "2   ECOPLANNING sprl      2019-212       NaN  101.930000  152882.850000   \n",
      "3   ECOPLANNING sprl      2019-207       NaN  101.889000  152892.925000   \n",
      "4   ECOPLANNING sprl      2019-214       NaN  101.854000  152888.082000   \n",
      "5   ECOPLANNING sprl      2019-217       NaN  101.815000  152886.185000   \n",
      "6   ECOPLANNING sprl      2019-225       NaN  101.669000  152881.112000   \n",
      "7                NaN           NaN       3.6  101.824000  152882.735000   \n",
      "8   ECOPLANNING sprl      2019-201       2.4  101.926886  152890.245758   \n",
      "9   ECOPLANNING sprl      2019-221       1.4  101.798666  152882.412821   \n",
      "10  ECOPLANNING sprl      2019-223       1.3  101.778308  152880.503593   \n",
      "11  ECOPLANNING sprl  2019-224 bis       0.4  101.744399  152883.150964   \n",
      "12  ECOPLANNING sprl      2019-224       2.4  101.689276  152883.047932   \n",
      "13  ECOPLANNING sprl      2019-219       1.5  101.846858  152890.597441   \n",
      "14  ECOPLANNING sprl      2019-220       0.5  101.846862  152880.561222   \n",
      "15  ECOPLANNING sprl  2019-220 bis       1.2  101.825243  152881.262319   \n",
      "16  ECOPLANNING sprl      2019-226       4.8  101.823793  152882.734621   \n",
      "\n",
      "      Type              Y  split_distinct  \n",
      "0   Forage  122594.620000           False  \n",
      "1   Forage  122592.986000           False  \n",
      "2   Forage  122591.453000           False  \n",
      "3   Forage  122592.662000           False  \n",
      "4   Forage  122588.486000           False  \n",
      "5   Forage  122587.152000           False  \n",
      "6   Forage  122580.962000           False  \n",
      "7   Forage  122586.452000           False  \n",
      "8   Forage  122596.474260           False  \n",
      "9   Forage  122584.570092           False  \n",
      "10  Forage  122583.142499           False  \n",
      "11  Forage  122582.644272           False  \n",
      "12  Forage  122582.093707           False  \n",
      "13  Forage  122585.539815           False  \n",
      "14  Forage  122586.761477           False  \n",
      "15  Forage  122586.508284           False  \n",
      "16  Forage  122586.451680           False  \n",
      "Ambiguous values present. Please resolve this manually !\n"
     ]
    }
   ],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "continuing-bridal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Long_for_x</th>\n",
       "      <th>Long_for_y</th>\n",
       "      <th>Source_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F205</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F208</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F212</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F207</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F214</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F217</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F225</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Long_for_x  Long_for_y  Source_index\n",
       "0  F205         3.2         4.8             0\n",
       "1  F208         3.4         4.8             1\n",
       "2  F212         3.4         4.8             2\n",
       "3  F207         3.4         4.8             3\n",
       "4  F214         3.6         4.8             4\n",
       "5  F217         4.2         4.8             5\n",
       "6  F225         4.0         4.8             6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescribed-block",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 17, columns : 20, Unique col 'ID': 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0daa93946348a6a790753c1e5e3997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='rows', max=17, min=10, readout=False), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-transport",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "criminal-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chronic-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 : database_Memoris3/Profils_sol_Boreholes.csv \n",
      "df2 : profils_sols_donnees_forages/donnees_forage_Boreholes.csv\n",
      "\n",
      "df1 - No position data\n",
      "df2 - Position data\n",
      "Rows : 826, columns : 6, Unique col 'ID': 172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9763b47581b0403d8de94356a0adf6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=826, min=3, readout=False), IntSlider(value=6…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows : 13, columns : 13, Unique col 'ID': 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1b266689d44d8cb3ee0872d15aac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='rows', max=13, min=3, readout=False), IntSlider(value=12…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protecting-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot proceed ! No position data in one of the dataframe\n"
     ]
    }
   ],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aggregate-tower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index         Type_refus  Long_for           Z              X Refus  \\\n",
      "0        0                NaN       NaN         NaN            NaN   NaN   \n",
      "1        1                NaN       NaN         NaN            NaN   NaN   \n",
      "2        2                NaN       NaN         NaN            NaN   NaN   \n",
      "3        3                NaN       NaN         NaN            NaN   NaN   \n",
      "4        4                NaN       NaN         NaN            NaN   NaN   \n",
      "..     ...                ...       ...         ...            ...   ...   \n",
      "834    834              Béton       0.8  103.207928  152838.481267     x   \n",
      "835    835              Béton       0.5  103.182578  152840.031071     x   \n",
      "836    836              Béton       0.8  103.225362  152871.977024     x   \n",
      "837    837  Matériaux indurés       1.4  103.253143  152874.285127     x   \n",
      "838    838  Matériaux indurés       1.4  103.246920  152874.975616     x   \n",
      "\n",
      "         ID  Diam_for              Y     Method    Type    Date_ouv  \\\n",
      "0        F2       NaN            NaN  carrotier  Forage  2010-03-11   \n",
      "1        F2       NaN            NaN  carrotier  Forage  2010-03-11   \n",
      "2        F2       NaN            NaN  carrotier  Forage  2010-03-11   \n",
      "3        F2       NaN            NaN  carrotier  Forage  2010-03-11   \n",
      "4        F2       NaN            NaN  carrotier  Forage  2010-03-11   \n",
      "..      ...       ...            ...        ...     ...         ...   \n",
      "834   F15aM      75.0  122651.360268  Dual tube  Forage  2017-02-23   \n",
      "835  F15a'M      75.0  122651.243036  Dual tube  Forage  2017-02-23   \n",
      "836   F17aM      75.0  122652.315890  Dual tube  Forage  2017-02-23   \n",
      "837   F17bM      75.0  122648.828238  Dual tube  Forage  2017-02-23   \n",
      "838   F17cM      75.0  122645.749197  Dual tube  Forage  2017-02-23   \n",
      "\n",
      "               Societe      ID_date  split_distinct  \n",
      "0    SBS Environnement      2010-F2           False  \n",
      "1    SBS Environnement      2010-F2           False  \n",
      "2    SBS Environnement      2010-F2           False  \n",
      "3    SBS Environnement      2010-F2           False  \n",
      "4    SBS Environnement      2010-F2           False  \n",
      "..                 ...          ...             ...  \n",
      "834   ECOPLANNING sprl   2017-F15aM           False  \n",
      "835   ECOPLANNING sprl  2017-F15a'M           False  \n",
      "836   ECOPLANNING sprl   2017-F17aM           False  \n",
      "837   ECOPLANNING sprl   2017-F17bM           False  \n",
      "838   ECOPLANNING sprl   2017-F17cM           False  \n",
      "\n",
      "[839 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "major-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['ID_date'] = mdf['ID_date'].apply(lambda x: str(x).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepted-chicken",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Profondeur'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-376f5be1ebe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Long_for'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Profondeur'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Long_for'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Profondeur'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-tJb-HgI6/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-tJb-HgI6/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/GSDMA-tJb-HgI6/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Profondeur'] not in index\""
     ]
    }
   ],
   "source": [
    "mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-chicken",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "exterior-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes.drop('index', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adverse-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes.drop('split_distinct', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "serious-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  Crep_long    Methode     Method     Resp_chantier  Diam_ext_pz  \\\n",
      "0        0        NaN  Dual tube   Liner_60  Liza Niemirowski          NaN   \n",
      "1        1        NaN  Dual tube   Liner_60  Liza Niemirowski          NaN   \n",
      "2        2        NaN  Dual tube        NaN  Liza Niemirowski          NaN   \n",
      "3        3        NaN  Dual tube   Gouge_75  Liza Niemirowski          NaN   \n",
      "4        4        NaN  Dual tube   Gouge_75  Liza Niemirowski          NaN   \n",
      "..     ...        ...        ...        ...               ...          ...   \n",
      "847    847        NaN        NaN  Dual tube               NaN          NaN   \n",
      "848    848        NaN        NaN  Dual tube               NaN          NaN   \n",
      "849    849        NaN        NaN  Dual tube               NaN          NaN   \n",
      "850    850        NaN        NaN  Dual tube               NaN          NaN   \n",
      "851    851        NaN        NaN  Dual tube               NaN          NaN   \n",
      "\n",
      "         ID  Long_pz  Diam_int_pz         Type_refus  ...           Z  Refus  \\\n",
      "0      F205      NaN          NaN                NaN  ...  101.804000    NaN   \n",
      "1      F205      NaN          NaN                NaN  ...  101.804000    NaN   \n",
      "2      F208      NaN          NaN                NaN  ...  101.848000    NaN   \n",
      "3      F212      NaN          NaN                NaN  ...  101.930000    NaN   \n",
      "4      F212      NaN          NaN                NaN  ...  101.930000    NaN   \n",
      "..      ...      ...          ...                ...  ...         ...    ...   \n",
      "847   F15aM      NaN          NaN              Béton  ...  103.207928      x   \n",
      "848  F15a'M      NaN          NaN              Béton  ...  103.182578      x   \n",
      "849   F17aM      NaN          NaN              Béton  ...  103.225362      x   \n",
      "850   F17bM      NaN          NaN  Matériaux indurés  ...  103.253143      x   \n",
      "851   F17cM      NaN          NaN  Matériaux indurés  ...  103.246920      x   \n",
      "\n",
      "                 X  Diam_for    Type    Date_ouv              Y  \\\n",
      "0    152887.693000      75.0  Forage   #conflict  122594.620000   \n",
      "1    152887.693000      75.0  Forage   #conflict  122594.620000   \n",
      "2    152885.296000      75.0  Forage  2019-10-07  122592.986000   \n",
      "3    152882.850000      75.0  Forage   #conflict  122591.453000   \n",
      "4    152882.850000      75.0  Forage   #conflict  122591.453000   \n",
      "..             ...       ...     ...         ...            ...   \n",
      "847  152838.481267      75.0  Forage  2017-02-23  122651.360268   \n",
      "848  152840.031071      75.0  Forage  2017-02-23  122651.243036   \n",
      "849  152871.977024      75.0  Forage  2017-02-23  122652.315890   \n",
      "850  152874.285127      75.0  Forage  2017-02-23  122648.828238   \n",
      "851  152874.975616      75.0  Forage  2017-02-23  122645.749197   \n",
      "\n",
      "              Societe      ID_date split_distinct  \n",
      "0           #conflict    #conflict          False  \n",
      "1           #conflict    #conflict          False  \n",
      "2    ECOPLANNING sprl     2019-208          False  \n",
      "3           #conflict    #conflict          False  \n",
      "4           #conflict    #conflict          False  \n",
      "..                ...          ...            ...  \n",
      "847  ECOPLANNING sprl   2017-F15AM          False  \n",
      "848  ECOPLANNING sprl  2017-F15A'M          False  \n",
      "849  ECOPLANNING sprl   2017-F17AM          False  \n",
      "850  ECOPLANNING sprl   2017-F17BM          False  \n",
      "851  ECOPLANNING sprl   2017-F17CM          False  \n",
      "\n",
      "[852 rows x 21 columns]\n",
      "Ambiguous values present. Please resolve this manually !\n"
     ]
    }
   ],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID', dist_max=2)\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "manual-subdivision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Crep_long</th>\n",
       "      <th>Methode</th>\n",
       "      <th>Method</th>\n",
       "      <th>Resp_chantier</th>\n",
       "      <th>Diam_ext_pz</th>\n",
       "      <th>ID</th>\n",
       "      <th>Long_pz</th>\n",
       "      <th>Diam_int_pz</th>\n",
       "      <th>Type_refus</th>\n",
       "      <th>...</th>\n",
       "      <th>Z</th>\n",
       "      <th>Refus</th>\n",
       "      <th>X</th>\n",
       "      <th>Diam_for</th>\n",
       "      <th>Type</th>\n",
       "      <th>Date_ouv</th>\n",
       "      <th>Y</th>\n",
       "      <th>Societe</th>\n",
       "      <th>ID_date</th>\n",
       "      <th>split_distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dual tube</td>\n",
       "      <td>tarrière</td>\n",
       "      <td>Liza Niemirowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>101.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152886.185</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Forage</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>122587.152</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dual tube</td>\n",
       "      <td>tarrière</td>\n",
       "      <td>Liza Niemirowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>101.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152886.185</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Forage</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>122587.152</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dual tube</td>\n",
       "      <td>tarrière</td>\n",
       "      <td>Liza Niemirowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>101.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152886.185</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Forage</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>122587.152</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dual tube</td>\n",
       "      <td>tarrière</td>\n",
       "      <td>Liza Niemirowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>101.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152886.185</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Forage</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>122587.152</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dual tube</td>\n",
       "      <td>tarrière</td>\n",
       "      <td>Liza Niemirowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>101.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152886.185</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Forage</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>122587.152</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>#conflict</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Crep_long    Methode    Method     Resp_chantier  Diam_ext_pz  \\\n",
       "12   12.0        NaN  Dual tube  tarrière  Liza Niemirowski          NaN   \n",
       "13   13.0        NaN  Dual tube  tarrière  Liza Niemirowski          NaN   \n",
       "14   14.0        NaN  Dual tube  tarrière  Liza Niemirowski          NaN   \n",
       "15   15.0        NaN  Dual tube  tarrière  Liza Niemirowski          NaN   \n",
       "16   16.0        NaN  Dual tube  tarrière  Liza Niemirowski          NaN   \n",
       "\n",
       "      ID  Long_pz  Diam_int_pz Type_refus  ...        Z  Refus           X  \\\n",
       "12  F217      NaN          NaN        NaN  ...  101.815    NaN  152886.185   \n",
       "13  F217      NaN          NaN        NaN  ...  101.815    NaN  152886.185   \n",
       "14  F217      NaN          NaN        NaN  ...  101.815    NaN  152886.185   \n",
       "15  F217      NaN          NaN        NaN  ...  101.815    NaN  152886.185   \n",
       "16  F217      NaN          NaN        NaN  ...  101.815    NaN  152886.185   \n",
       "\n",
       "    Diam_for    Type   Date_ouv           Y    Societe    ID_date  \\\n",
       "12      75.0  Forage  #conflict  122587.152  #conflict  #conflict   \n",
       "13      75.0  Forage  #conflict  122587.152  #conflict  #conflict   \n",
       "14      75.0  Forage  #conflict  122587.152  #conflict  #conflict   \n",
       "15      75.0  Forage  #conflict  122587.152  #conflict  #conflict   \n",
       "16      75.0  Forage  #conflict  122587.152  #conflict  #conflict   \n",
       "\n",
       "   split_distinct  \n",
       "12          False  \n",
       "13          False  \n",
       "14          False  \n",
       "15          False  \n",
       "16          False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boreholes.loc[[12,13,14,15,16],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wound-heavy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date_ouv_x</th>\n",
       "      <th>Date_ouv_y</th>\n",
       "      <th>Societe_x</th>\n",
       "      <th>Societe_y</th>\n",
       "      <th>ID_date_x</th>\n",
       "      <th>ID_date_y</th>\n",
       "      <th>Source_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F205</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-205</td>\n",
       "      <td>2010-F205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F205</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-205</td>\n",
       "      <td>2010-F205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F212</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-05-10</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SITEREM</td>\n",
       "      <td>2019-212</td>\n",
       "      <td>2010-F212</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F217</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-02</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-217</td>\n",
       "      <td>2010-F217</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>F217</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-02</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-217</td>\n",
       "      <td>2010-F217</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F217</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-02</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-217</td>\n",
       "      <td>2010-F217</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>F217</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-02</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-217</td>\n",
       "      <td>2010-F217</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>F217</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>2010-03-02</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-217</td>\n",
       "      <td>2010-F217</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>F219</td>\n",
       "      <td>2019-10-08</td>\n",
       "      <td>2010-03-03</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-219</td>\n",
       "      <td>2010-F219</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>F219</td>\n",
       "      <td>2019-10-08</td>\n",
       "      <td>2010-03-03</td>\n",
       "      <td>ECOPLANNING sprl</td>\n",
       "      <td>SBS Environnement</td>\n",
       "      <td>2019-219</td>\n",
       "      <td>2010-F219</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Date_ouv_x  Date_ouv_y         Societe_x          Societe_y  \\\n",
       "0   F205  2019-10-07  2010-03-26  ECOPLANNING sprl            SITEREM   \n",
       "1   F205  2019-10-07  2010-03-26  ECOPLANNING sprl            SITEREM   \n",
       "3   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "4   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "5   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "6   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "7   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "8   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "9   F212  2019-10-07  2010-05-10  ECOPLANNING sprl            SITEREM   \n",
       "12  F217  2019-10-07  2010-03-02  ECOPLANNING sprl  SBS Environnement   \n",
       "13  F217  2019-10-07  2010-03-02  ECOPLANNING sprl  SBS Environnement   \n",
       "14  F217  2019-10-07  2010-03-02  ECOPLANNING sprl  SBS Environnement   \n",
       "15  F217  2019-10-07  2010-03-02  ECOPLANNING sprl  SBS Environnement   \n",
       "16  F217  2019-10-07  2010-03-02  ECOPLANNING sprl  SBS Environnement   \n",
       "24  F219  2019-10-08  2010-03-03  ECOPLANNING sprl  SBS Environnement   \n",
       "25  F219  2019-10-08  2010-03-03  ECOPLANNING sprl  SBS Environnement   \n",
       "\n",
       "   ID_date_x  ID_date_y  Source_index  \n",
       "0   2019-205  2010-F205             0  \n",
       "1   2019-205  2010-F205             1  \n",
       "3   2019-212  2010-F212             3  \n",
       "4   2019-212  2010-F212             4  \n",
       "5   2019-212  2010-F212             5  \n",
       "6   2019-212  2010-F212             6  \n",
       "7   2019-212  2010-F212             7  \n",
       "8   2019-212  2010-F212             8  \n",
       "9   2019-212  2010-F212             9  \n",
       "12  2019-217  2010-F217            12  \n",
       "13  2019-217  2010-F217            13  \n",
       "14  2019-217  2010-F217            14  \n",
       "15  2019-217  2010-F217            15  \n",
       "16  2019-217  2010-F217            16  \n",
       "24  2019-219  2010-F219            24  \n",
       "25  2019-219  2010-F219            25  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(boreholes, err_df, on='ID', col='Long_for', idx_list=[0,1,2], valid_col='Long_for_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-classics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][0]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', )\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-hours",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-housing",
   "metadata": {},
   "source": [
    "#### Last boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-feature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID')\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-press",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_df # i think there are not the same, but no date or postition to distinguish them !\n",
    "# --> check boreholes sheets (pdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hungarian-partition",
   "metadata": {},
   "source": [
    "validate_data(boreholes, err_df, on='ID', col='Long_for', idx_list=[], valid_col='Long_for_y' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "parliamentary-newark",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-portrait",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Boreholes data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "boreholes.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-flexibility",
   "metadata": {},
   "source": [
    "# Piezometers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-usage",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Piezometer'\n",
    "save_file = f'Merged_Piezometers.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "piezometers = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-centre",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-garden",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-county",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-associate",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-burning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-midwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-chicago",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-sentence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-tyler",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [292, 293]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-shanghai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][4]\n",
    "file2= files_dict[key][5]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-essex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-duplicate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-whale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-differential",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][6]\n",
    "file2= files_dict[key][9]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ID'] = df2.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-dress",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-pulse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-ceramic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][10]\n",
    "file2= files_dict[key][11]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ID'] = df1.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-ireland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-catalog",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-battery",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "legislative-financing",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-broadcasting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-reserve",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][12]\n",
    "file2= files_dict[key][13]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-tuning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adjustable-marker",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "realistic-myrtle",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-geography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-sheffield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-stomach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][14]\n",
    "file2= files_dict[key][15]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-holder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-grove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "retained-marking",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "floral-script",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-lexington",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-colleague",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][16]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-stack",
   "metadata": {},
   "source": [
    "#### Last merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-alberta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "median-portrait",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "elder-weekly",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-dairy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lasting-issue",
   "metadata": {},
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "comparable-transformation",
   "metadata": {},
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "illegal-going",
   "metadata": {},
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-shock",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Piezometers data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "piezometers.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-orchestra",
   "metadata": {},
   "source": [
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-ranch",
   "metadata": {},
   "source": [
    "# Unknown facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='facility'\n",
    "save_file = f'Merged_Facilites_unknw.csv'\n",
    "#coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "facilities = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-beginning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-metabolism",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][1]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-listing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facilities, err_df=gdf_merger(facilities, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-campbell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(facilities, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-asset",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Unknown Facilities data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "facilities.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-clearance",
   "metadata": {},
   "source": [
    "# Lithologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-timeline",
   "metadata": {},
   "source": [
    "Do not add parameter 'dist_max' when merging without considering position !!! otherwise, unuseless rows added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Litho'\n",
    "save_file = f'Merged_Lithologies.csv'\n",
    "coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "lithologies = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-coverage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-catholic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = list(set(df1.columns) & set(df2.columns))\n",
    "test1 = df1.merge(df2, how = 'inner', on='ID')\n",
    "test2 = df1.merge(df2, how = 'outer', on='ID', indicator=True).loc[lambda x : x.query('_merge ==\"right_only\" or _merge==\"left_only\"').index]\n",
    "test3 = test1.merge(test2, how = 'outer', on='ID')\n",
    "test4 = df1.merge(df2, how = 'outer', on=list(common_cols))\n",
    "print((len(test1), len(test2), len(test3)))\n",
    "gdf_viewer(test4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wired-trade",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "yellow-medicare",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-manitoba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-snake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "respected-angle",
   "metadata": {},
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "structural-express",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-intelligence",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-preview",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-responsibility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=10, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-measure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][6]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how='inner', on='ID'), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how = 'outer', on='ID',indicator=True), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-medicaid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "initial-piano",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governing-spider",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-lodging",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-flower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "changing-accident",
   "metadata": {},
   "source": [
    "mdf = mdf.loc[mdf.query('Terrain==Terrain').index,:]\n",
    "mdf.drop(columns=['Terrain_x', 'Terrain_y'], inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lovely-ballot",
   "metadata": {},
   "source": [
    "data = mdf.copy()\n",
    "col = 'Terrain'\n",
    "\n",
    "id_list = []\n",
    "keep_idx = []\n",
    "\n",
    "for i in data.index:\n",
    "    _id = data.loc[i,'ID']\n",
    "    if re.search(\"'\", _id): \n",
    "        _id = f\"`{_id}`\" # it doesn't work \n",
    "    else:\n",
    "        _id = f\"{_id}\"\n",
    "    \n",
    "    if _id not in id_list:\n",
    "        id_list.append(_id)\n",
    "        tmp = data[data['ID']==f\"{_id}\"]\n",
    "\n",
    "        if len(tmp) < 2 and len(tmp) > 0:\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "        else:\n",
    "            tmp = tmp[tmp[col]==tmp[col]]\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "print(keep_idx)\n",
    "    \n",
    "data = data.loc[keep_idx,:]\n",
    "data.drop(columns=[col+'_x', col+'_y'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "chronic-greek",
   "metadata": {},
   "source": [
    "df1.drop(columns=['Societe','Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "warming-cleaner",
   "metadata": {},
   "source": [
    "df1['X'] = df1['X'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Y'] = df1['Y'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Ep_remb'] = df1['Ep_remb'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governmental-spyware",
   "metadata": {},
   "source": [
    "df1['Type'] = df1['Type'].astype('object')\n",
    "df1[['X','Y','Ep_remb']] = df1[['X','Y','Ep_remb']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-precipitation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-courage",
   "metadata": {},
   "source": [
    "# Equipments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-thesis",
   "metadata": {},
   "source": [
    "We must also retrieve equipments information from boreholes and piezometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Equipm'\n",
    "save_file = f'Merged_Equipments.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "equipments = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-garlic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-battlefield",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-doubt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-speaker",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-desire",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-density",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "promotional-joint",
   "metadata": {},
   "source": [
    "key='Samples'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "second-springfield",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "manual-talent",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrong-jersey",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-blood",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-chapel",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dental-hello",
   "metadata": {},
   "source": [
    "key='Measures'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "european-breast",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-tomato",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "optimum-concern",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-sewing",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-winner",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "productive-wheat",
   "metadata": {},
   "source": [
    "key='Analysis'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vulnerable-encounter",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "genuine-fraction",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "married-beatles",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-applicant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
