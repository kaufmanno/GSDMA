{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-killing",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import gen_id_dated, gdf_viewer, gdf_geom, gdf_merger, gdf_filter, na_col_drop, na_line_drop\n",
    "import re, os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "#from shapely.geometry import Point\n",
    "import datetime as dtm\n",
    "import matplotlib.pyplot as plt\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-pride",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def files_search(work_dir, files_dict, prefix='', skip=None, details=False):   \n",
    "    \n",
    "    if skip is None:\n",
    "        skip = \"we don't want to skip a word\"\n",
    "        \n",
    "    for k in files_dict.keys():\n",
    "        tmp_list = []\n",
    "        for p,d,f in os.walk(work_dir):\n",
    "            for x in f:\n",
    "                add = False\n",
    "                if re.search(prefix,x,re.I) and not re.search(skip,x,re.I):\n",
    "                    add = True\n",
    "                    i = str(f'{p}/{x}')\n",
    "                else:\n",
    "                    add = False\n",
    "                    i=''\n",
    "                    \n",
    "                if re.search(k,i,re.I) and add:\n",
    "                    tmp_list.append(i)\n",
    "        tmp_list.sort()\n",
    "        files_dict.update({k:tmp_list})\n",
    "\n",
    "    for k,v in files_dict.items():\n",
    "        print(k,' \\t: ',len(v))\n",
    "    \n",
    "    if details: # Look filenames\n",
    "        which = files_dict.keys()\n",
    "\n",
    "        for w in which:\n",
    "            print('\\n+++++++++++++++++')\n",
    "            print(f'+  {w.upper()}\\t+ ')\n",
    "            print('+++++++++++++++++')\n",
    "            [print(i, '-', x) for i, x in enumerate(files_dict[w], 0)]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_col(data):\n",
    "    cols_idx = []\n",
    "    \n",
    "    class DoubledColumns(Exception):\n",
    "        \"\"\"Merging process doubled column(s) still remain. Check and drop them before continue\"\"\"\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(data.columns)):\n",
    "        if re.search('_x|_y', list(data.columns)[i]):\n",
    "            cols_idx.append(i)\n",
    "    \n",
    "    if len(cols_idx) != 0 :\n",
    "        raise DoubledColumns(f'Merging process doubled column(s) still remain.'\n",
    "                             f'\\nCheck and drop them before continue ! Doubled columns position {cols_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_obj_test(df1, df2, on='ID', how='outer', dist_max=1):\n",
    "    test_distinct = df1.merge(df2, on=on, how=how)\n",
    "\n",
    "    dist_max = dist_max\n",
    "    \n",
    "    if 'X' in list(df1.columns) and 'X' in list(df2.columns):\n",
    "        for idx in test_distinct.index:\n",
    "            distinct_objects = True\n",
    "            if not pd.isnull(test_distinct.loc[idx,'X_x']) and not pd.isnull(test_distinct.loc[idx,'X_y']):\n",
    "                dist = (test_distinct.loc[idx,'X_x'] - test_distinct.loc[idx,'X_y']) ** 2 + (test_distinct.loc[idx,'Y_x'] - test_distinct.loc[idx,'Y_y']) ** 2\n",
    "                if dist <= (dist_max) ** 2:  # consider as same object\n",
    "                    distinct_objects = False\n",
    "            else:\n",
    "                distinct_objects = False\n",
    "            test_distinct.loc[idx, 'Distinct_obj'] = distinct_objects\n",
    "\n",
    "        test_distinct.insert(1,'Distinct_obj', test_distinct.pop('Distinct_obj') )\n",
    "\n",
    "        gdf_viewer(test_distinct)\n",
    "    else:\n",
    "        print('Cannot proceed ! No position data in one of the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file1, file2): # find another name for this function\n",
    "    \"\"\"\n",
    "    create dataframes from files and test if they contain position informations\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_csv(file1, delimiter=',')\n",
    "    df2 = pd.read_csv(file2, delimiter=',')\n",
    "    \n",
    "    print(f\"df1 : {file1.replace(work_dir,'')} \\ndf2 : {file2.replace(work_dir,'')}\\n\")\n",
    "\n",
    "    if 'X' in list(df1.columns): print('df1 - Position data')\n",
    "    else: print('df1 - No position data')\n",
    "    if 'X' in list(df2.columns): print('df2 - Position data')\n",
    "    else: print('df2 - No position data')\n",
    "    \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, data_to_check, valid_data, col, idx_list, valid_col):\n",
    "    \n",
    "    old_idx_col = 'Source_index'\n",
    "    \n",
    "    for col, idx_list in valid_data.items():\n",
    "        if old_idx_col in data_to_check.columns:\n",
    "            idx = data_to_check.loc[i, old_idx_col]\n",
    "            data.loc[idx, col] = data_to_check.loc[i, valid_col]\n",
    "        else:\n",
    "            raise NameError(f\"Dataframe to check must contain a column named : '{old_idx_col}'!\")\n",
    "\n",
    "    data_to_check.drop(index=idx_list, inplace=True)\n",
    "    data_to_check.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Operation done \")\n",
    "    \n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_objects_check(data):\n",
    "    uniq_ID = []\n",
    "    dbl_ID = []\n",
    "    idx_ = []\n",
    "    qdf = pd.DataFrame()\n",
    "\n",
    "    for i in data.index:\n",
    "        id_ = data.loc[i, 'ID']\n",
    "\n",
    "        if id_ not in uniq_ID:\n",
    "            uniq_ID.append(id_)\n",
    "        elif id_ not in dbl_ID:\n",
    "            dbl_ID.append(id_)\n",
    "        else:\n",
    "            idx_.append(i)\n",
    "\n",
    "    for i in dbl_ID:\n",
    "        qdf = qdf.append(data.query(f\"ID=='{i}'\"))\n",
    "\n",
    "    return qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-noise",
   "metadata": {},
   "source": [
    "## Files reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = ROOT_DIR+'/CF_data/Result_traitem/'\n",
    "save_dir = ROOT_DIR+'/CF_data/Donnees_fusionnees/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my dictionary structure to retrieve good files (Keynames !!!)\n",
    "files_dict={'Borehole':0,'Piezometer':0,'Piezair':0,'Trench':0,'Litho':0,'Equipm':0,\n",
    "        'Measure':0,'Sample':0,'Analysis':0,'facility':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-sensitivity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_search(work_dir, files_dict, prefix='', skip='source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "how=['inner', 'outer', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = False\n",
    "t = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-transformation",
   "metadata": {},
   "source": [
    "# Boreholes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-oregon",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- correct extraction in the file 2 -> Samples\n",
    "- file 4 and file 5 are the same in result (check it)\n",
    "- try to concatenate file 1 with piezo (if possible because no position)\n",
    "- check processing for 'refus and 'type_refus' (every object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys: Borehole','Piezometer','Litho', 'Trench','Equipm','Measure','Sample','Analysis','facility'\n",
    "files_dict['Borehole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Borehole'\n",
    "save_file = f'Merged_Boreholes.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus', 'Societe'] #columns of interest\n",
    "boreholes = pd.DataFrame() # for saving object info after last merging\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-stranger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][1]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.ID=df2.ID.apply(lambda x: 'F'+x) # name recent (2019) boreholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-block",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-transport",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-tower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['ID_date'] = mdf['ID_date'].apply(lambda x: str(x).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "# mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-chicken",
   "metadata": {},
   "source": [
    "#### boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes.drop('index', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes.drop('split_distinct', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID', dist_max=2)\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "boreholes.loc[[12,13,14,15,16],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Continue to write validate data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(boreholes, err_df, valid_data={'Date_ouv_x': [0]}, col='Long_for', idx_list=[0,1,2], valid_col='Long_for_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e74a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-classics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][0]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_obj_test(df1, df2, dist_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', )\n",
    "check_col(mdf) # check if columns with '..._x' or '..._y' are still present and raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['Long_for'] = mdf[['Profondeur', 'Long_for']].apply(lambda x: x[0] if pd.isnull(x[1]) else x[1], axis=1)\n",
    "mdf.drop(columns=['Profondeur'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-hours",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=10, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-housing",
   "metadata": {},
   "source": [
    "#### Last boreholes merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-feature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boreholes, err_df=gdf_merger(boreholes, mdf, how=how[1], on='ID')\n",
    "check_col(boreholes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-press",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_df # i think there are not the same, but no date or postition to distinguish them !\n",
    "# --> check boreholes sheets (pdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hungarian-partition",
   "metadata": {},
   "source": [
    "validate_data(boreholes, err_df, on='ID', col='Long_for', idx_list=[], valid_col='Long_for_y' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "parliamentary-newark",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(boreholes, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-portrait",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Boreholes data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "boreholes.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-flexibility",
   "metadata": {},
   "source": [
    "# Piezometers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-usage",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Piezometer'\n",
    "save_file = f'Merged_Piezometers.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "piezometers = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-centre",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-garden",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-county",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-associate",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-burning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-midwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-chicago",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-sentence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-tyler",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [292, 293]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-shanghai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][4]\n",
    "file2= files_dict[key][5]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-seller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-essex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-duplicate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-whale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-differential",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][6]\n",
    "file2= files_dict[key][9]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ID'] = df2.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-dress",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-pulse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-ceramic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][10]\n",
    "file2= files_dict[key][11]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ID'] = df1.ID.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-ireland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-catalog",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-battery",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "legislative-financing",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-broadcasting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-reserve",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][12]\n",
    "file2= files_dict[key][13]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-tuning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adjustable-marker",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "realistic-myrtle",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-geography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-sheffield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-stomach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][14]\n",
    "file2= files_dict[key][15]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-holder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-grove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "retained-marking",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "floral-script",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-lexington",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-colleague",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][16]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-stack",
   "metadata": {},
   "source": [
    "#### Last merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-alberta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piezometers, err_df=gdf_merger(piezometers, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "median-portrait",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "elder-weekly",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-dairy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(piezometers, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lasting-issue",
   "metadata": {},
   "source": [
    "piezometers, check = gdf_filter(piezometers, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True)\n",
    "#gdf_viewer(piezometers, rows=5, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "comparable-transformation",
   "metadata": {},
   "source": [
    "double_objects_check(piezometers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "illegal-going",
   "metadata": {},
   "source": [
    "drop_id = [2,4,30,94,106]\n",
    "piezometers.drop(index=drop_id, inplace=True)\n",
    "gdf_viewer(piezometers, rows=5, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-shock",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Piezometers data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "piezometers.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-orchestra",
   "metadata": {},
   "source": [
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-ranch",
   "metadata": {},
   "source": [
    "# Unknown facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='facility'\n",
    "save_file = f'Merged_Facilites_unknw.csv'\n",
    "#coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "facilities = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-beginning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-metabolism",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][1]\n",
    "df1 = pd.read_csv(file1, delimiter=',')\n",
    "\n",
    "print(f\"df1 : {file1.replace(work_dir,'')}\")\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-listing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facilities, err_df=gdf_merger(facilities, df1, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-campbell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(facilities, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-asset",
   "metadata": {},
   "source": [
    "####  $\\color{red}{\\textbf{Save final Unknown Facilities data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "facilities.to_csv(save_dir+save_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-clearance",
   "metadata": {},
   "source": [
    "# Lithologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-timeline",
   "metadata": {},
   "source": [
    "Do not add parameter 'dist_max' when merging without considering position !!! otherwise, unuseless rows added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Litho'\n",
    "save_file = f'Merged_Lithologies.csv'\n",
    "coi=['ID','X','Y','Z','Litho_top','Litho_base','Description']  #columns of interest\n",
    "lithologies = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-coverage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][3]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')#, step_merge\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-catholic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = list(set(df1.columns) & set(df2.columns))\n",
    "test1 = df1.merge(df2, how = 'inner', on='ID')\n",
    "test2 = df1.merge(df2, how = 'outer', on='ID', indicator=True).loc[lambda x : x.query('_merge ==\"right_only\" or _merge==\"left_only\"').index]\n",
    "test3 = test1.merge(test2, how = 'outer', on='ID')\n",
    "test4 = df1.merge(df2, how = 'outer', on=list(common_cols))\n",
    "print((len(test1), len(test2), len(test3)))\n",
    "gdf_viewer(test4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wired-trade",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "yellow-medicare",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-manitoba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][2]\n",
    "file2= files_dict[key][4]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-snake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "respected-angle",
   "metadata": {},
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "structural-express",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-intelligence",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-preview",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID')\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-responsibility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=10, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-measure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][5]\n",
    "file2= files_dict[key][6]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how='inner', on='ID'), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(df1.merge(df2, how = 'outer', on='ID',indicator=True), rows=5, cols=15, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-medicaid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_viewer(err_df, rows=5, un_val='ID', view=t) #err_df.ID.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "initial-piano",
   "metadata": {},
   "source": [
    "validate_data(piezometers, err_df, on='ID', col='Diam_ext_pz', id_list=[], valid_col='Diam_ext_pz_x' )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governing-spider",
   "metadata": {},
   "source": [
    "err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-lodging",
   "metadata": {},
   "source": [
    "##### Lithologies merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies, err_df=gdf_merger(lithologies, mdf, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-flower",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(lithologies, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "changing-accident",
   "metadata": {},
   "source": [
    "mdf = mdf.loc[mdf.query('Terrain==Terrain').index,:]\n",
    "mdf.drop(columns=['Terrain_x', 'Terrain_y'], inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lovely-ballot",
   "metadata": {},
   "source": [
    "data = mdf.copy()\n",
    "col = 'Terrain'\n",
    "\n",
    "id_list = []\n",
    "keep_idx = []\n",
    "\n",
    "for i in data.index:\n",
    "    _id = data.loc[i,'ID']\n",
    "    if re.search(\"'\", _id): \n",
    "        _id = f\"`{_id}`\" # it doesn't work \n",
    "    else:\n",
    "        _id = f\"{_id}\"\n",
    "    \n",
    "    if _id not in id_list:\n",
    "        id_list.append(_id)\n",
    "        tmp = data[data['ID']==f\"{_id}\"]\n",
    "\n",
    "        if len(tmp) < 2 and len(tmp) > 0:\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "        else:\n",
    "            tmp = tmp[tmp[col]==tmp[col]]\n",
    "            keep_idx = keep_idx + list(tmp.index)\n",
    "print(keep_idx)\n",
    "    \n",
    "data = data.loc[keep_idx,:]\n",
    "data.drop(columns=[col+'_x', col+'_y'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "chronic-greek",
   "metadata": {},
   "source": [
    "df1.drop(columns=['Societe','Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "warming-cleaner",
   "metadata": {},
   "source": [
    "df1['X'] = df1['X'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Y'] = df1['Y'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)\n",
    "df1['Ep_remb'] = df1['Ep_remb'].apply(lambda v : re.sub(',','.',v) if not pd.isnull(v) else v)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governmental-spyware",
   "metadata": {},
   "source": [
    "df1['Type'] = df1['Type'].astype('object')\n",
    "df1[['X','Y','Ep_remb']] = df1[['X','Y','Ep_remb']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-precipitation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-courage",
   "metadata": {},
   "source": [
    "# Equipments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-thesis",
   "metadata": {},
   "source": [
    "We must also retrieve equipments information from boreholes and piezometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "key='Equipm'\n",
    "save_file = f'Merged_Equipments.csv'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "equipments = pd.DataFrame()\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-garlic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file1= files_dict[key][0]\n",
    "file2= files_dict[key][1]\n",
    "\n",
    "df1, df2 = create_df(file1, file2)\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=1)\n",
    "check_col(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-battlefield",
   "metadata": {},
   "source": [
    "##### check and validate duplicate objects\n",
    "- The function \"gdf_filter()\" doesn't work in some cases, so we use function \"doubled_objects_check()\"\n",
    "- we have same objects Names but differents by positions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf, check = gdf_filter(mdf, position=True, id_on='ID', expression='sup|prof', dist_crit=1, drop=True, rapp_val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_objects_check(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_id = [2,25,30] # objects are seemingly the same, but is it possible to get 2 objects so close (~ 1m)?\n",
    "mdf.drop(index=drop_id, inplace=True)\n",
    "mdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-doubt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-speaker",
   "metadata": {},
   "source": [
    "##### Piezometers merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "piezometers = mdf.copy() #saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-desire",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-density",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "promotional-joint",
   "metadata": {},
   "source": [
    "key='Samples'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "second-springfield",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "manual-talent",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrong-jersey",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-blood",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-chapel",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dental-hello",
   "metadata": {},
   "source": [
    "key='Measures'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "european-breast",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "architectural-tomato",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "optimum-concern",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-sewing",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-winner",
   "metadata": {},
   "source": [
    "Some corrections todo in 'data organization':\n",
    "- file 0 and file 1 are the same in result (check it)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "productive-wheat",
   "metadata": {},
   "source": [
    "key='Analysis'\n",
    "coi=['ID','ID_date','X','Y','Z','Type','Long_for','Diam_for','Refus'] #columns of interest\n",
    "print(len(files_dict[key]), 'files')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vulnerable-encounter",
   "metadata": {},
   "source": [
    "ext_df = pd.read_csv(files_dict['Boreholes'][2], delimiter=',')\n",
    "gdf_viewer(ext_df, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "genuine-fraction",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = pd.read_csv(files_dict[key][0], delimiter=',')\n",
    "df2 = pd.read_csv(files_dict[key][2], delimiter=',')\n",
    "gdf_viewer(df1, rows=3, un_val='ID', view=t), gdf_viewer(df2, rows=3, un_val='ID', view=t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "married-beatles",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mdf, err_df=gdf_merger(df1, df2, how=how[1], on='ID', dist_max=2)\n",
    "gdf_viewer(mdf, rows=3, cols=13, un_val='ID', view=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-applicant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
